activation: ReLU
dropout: 0.2
hidden_dim: 32
input_chunk_length: 120
input_size: 5
likelihood: null
lr_scheduler_cls: !!python/name:torch.optim.lr_scheduler.ReduceLROnPlateau ''
lr_scheduler_kwargs:
  factor: 0.2
  min_lr: 1.0e-06
  mode: min
  monitor: val_loss
  patience: 5
  verbose: true
name: LSTM
nr_params: 1
num_layers: 2
num_layers_out_fc: []
optimizer_cls: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs:
  lr: 0.001
output_chunk_length: 30
output_chunk_shift: 0
target_size: 1
train_sample_shape:
- !!python/tuple
  - 120
  - 1
- !!python/tuple
  - 120
  - 4
- null
- null
- null
- !!python/tuple
  - 30
  - 1
use_reversible_instance_norm: false
